---
layout: post
title:      "'Beachcomber: My First CLI Gem'"
date:       2017-12-24 16:48:01 -0500
permalink:  beachcomber_my_first_cli_gem
---


Facing the monumental task of building my first Ruby Gem felt like an impossibily lofty goal, and at times one that was far beyond my grasp. As a fledgling Rubyist, I was being pushed out of the nest and asked to spread my wings and fly. I began my journey by thoroughly reading the documentation on Nokogiri and watching both recommended instructional videos.  Despite my pre-development efforts and the fact that Bundler did alot of the work for me, starting my first CLI gem from scratch was still an event filled with much anticipation and uncertainty.  Using Avi's system of writing out what I wanted my gem to do and envisioning how the user would interact with it made getting started alot easier.  I focused my efforts on writing out good, descriptive notes so that I could reference them as I continued to build out my project. It made it easier to think of my project as a series of smaller steps so that I could complete one goal at a time, adding form and functionality as I progressed. I started by identifying the classes that thought would be involved in my project.  I began by building a CLI class, which would act as my controller.  From there, I added perhaps my most important class, my Scraper class.  This is where things got a little complicated, because the website that I had initially chosen to scrape was replete with javascript and after two or three days and several failed attempts to scrape it I was forced to look elsewhere for a suitable source of data.  Lucky for me I found another web page rather quickly, and this time it was built with scraping in mind. The data for my project was readily exposed and ripe for the picking on every page! Beachcomber is a CLI application that allows the user to choose the state where they'd like to beachcomb, and then the beach, and it will provide the user with the high and low tide forcast for the month.  To facilitate it's operation, I began by scraping the index page of "www.tides.net" using a #Scraper class, and creating objects for each of the states and regions listed on the page. During creation, each #State object included a #name property, a #URL propertly, and an empty array named #beaches which I would later use to hold all of the Beach objects for the state.  Using each states URL, I appended the base URL of the index page and the states URL to scrape each specific #State page for the names and URL's of the beaches within each state.  Using this information, I created #Beach objects with a #name property and #URL property and assigned the #Beach objects to the instance array named #beaches within each State.  Scraping three separate pages for the information could have been troublesome, but the forthought of the webmasters made things go much more smoothly.
